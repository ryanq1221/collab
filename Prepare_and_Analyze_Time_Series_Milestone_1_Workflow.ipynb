{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryanq1221/collab/blob/main/Prepare_and_Analyze_Time_Series_Milestone_1_Workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxQpw7SxXf8z"
      },
      "source": [
        "# Prepare and Analyze Time Series - Milestone 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKbgeyIwXf9A"
      },
      "source": [
        "This Jupyter notebook serves as a guided workflow to Milestone 1 of the liveProject on End-to-end Time Series Forecasting with Deep Learning.\n",
        "\n",
        "You can upload this notebook to Colab and work from there. Alternatively, you can also work on this notebook in your local environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukllI4csXf9Z"
      },
      "source": [
        "Our aim in this Milestone is to perform preliminary data processing and cleaning to have preprocessed data ready, as shown by the red dotted box in the diagram below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rpLnPJCXf9p"
      },
      "source": [
        "![Milestone 1](https://s3.ap-southeast-1.amazonaws.com/www.jiahao.io/manning/project1_milestone_1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFQ8osYJXf95"
      },
      "source": [
        "## Importing Necessary Libraries and Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2aVi20nXf98"
      },
      "source": [
        "Let us first import the necessary libraries and load the data that we will be working with throughout this Milestone.  \n",
        "\n",
        "The data (data/sales.csv) that we are using is a daily retail sales dataset modified from the [M5 competition data](https://www.kaggle.com/c/m5-forecasting-accuracy/data). \n",
        "\n",
        "In this liveProject, you are a data scientist at a large retailer and your challenge is to forecast the sales of the respective stores by each category for the next 28 days."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InkTNb8CXf-R"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- We have written the code to import the libraries so you can just run it as-is. If you need other libraries while working on this notebook, please feel free to add the library to this cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wgcjBeLdXf-v"
      },
      "outputs": [],
      "source": [
        "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
        "\n",
        "# import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# suppress pandas SettingWithCopyWarning \n",
        "pd.options.mode.chained_assignment = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyOzqMQRXf_E"
      },
      "source": [
        "## Previewing the Sales Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suMJVf0kXf_s"
      },
      "source": [
        "Let us first have a preview of the data to understand what we will be working with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwo36BzbXf_y"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Read in the data *sales.csv* from the data folder into a pandas dataframe and preview the first 5 rows.\n",
        "\n",
        "<ins>Note</ins>:<br>\n",
        "- If you are running this notebook in Colab, please upload *sales.csv* from the data folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg60dfCctJLG"
      },
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- [Follow the example code here to upload files to Colab from your local file system](https://colab.research.google.com/notebooks/io.ipynb)\n",
        "- [Use Pandas `read_csv` function to load CSV data](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)\n",
        "- [Use Pandas `head` method to preview first 5 rows of dataframe `df`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "51d9EUZHdac_"
      },
      "outputs": [],
      "source": [
        "# upload file from Colab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7RpizKlwXf_1",
        "outputId": "b12191b4-d740-4136-c67d-ea10b5641386"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         date store_id cat_id    sales\n",
              "0  2011-01-29     TX_1  FOODS  3950.35\n",
              "1  2011-01-30     TX_1  FOODS  3844.97\n",
              "2  2011-01-31     TX_1  FOODS  2888.03\n",
              "3  2011-02-01     TX_1  FOODS  3631.28\n",
              "4  2011-02-02     TX_1  FOODS  3072.18"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1a169462-cf9e-48c7-a05d-eb88226d1454\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>store_id</th>\n",
              "      <th>cat_id</th>\n",
              "      <th>sales</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2011-01-29</td>\n",
              "      <td>TX_1</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>3950.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2011-01-30</td>\n",
              "      <td>TX_1</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>3844.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2011-01-31</td>\n",
              "      <td>TX_1</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>2888.03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2011-02-01</td>\n",
              "      <td>TX_1</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>3631.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2011-02-02</td>\n",
              "      <td>TX_1</td>\n",
              "      <td>FOODS</td>\n",
              "      <td>3072.18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1a169462-cf9e-48c7-a05d-eb88226d1454')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1a169462-cf9e-48c7-a05d-eb88226d1454 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1a169462-cf9e-48c7-a05d-eb88226d1454');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# load data and view first 5 rows\n",
        "sales_data = pd.read_csv('sales.csv') \n",
        "sales_data.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgFpkzJKXgAC"
      },
      "source": [
        "You should see from the data preview that there are 4 columns with ~58k rows. The daily sales (column: `sales`) for each store (column: `store_id`) and category (column: `cat_id`) are listed in each row. \n",
        "\n",
        "If we generate the unique value count of the `store_id` and `cat_id`, we will see that there are 10 stores and 3 categories. Each store and category group is a time series, so we have 30 time series."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2frgYX0sXgAK"
      },
      "source": [
        "<ins>Instruction</ins>:<br>\n",
        "- Verify the number of unique values of `store_id` and `cat_id` each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGnxNJ8NtJLI"
      },
      "source": [
        "<ins>Hints</ins>:<br>\n",
        "- [Pandas `nunique` method can be used to verify number of unique values](https://pandas.pydata.org/docs/reference/api/pandas.Series.nunique.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIdivPdbXgAO",
        "outputId": "b4ea4cc5-943c-4cb3-c109-ff315a4973e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "store_id    10\n",
            "cat_id       3\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# unique count of store_id and cat_id\n",
        "print(sales_data[['store_id','cat_id']].nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eTQF41fXgAP"
      },
      "source": [
        "And in terms of the time period, our sales data start from 2011 to 2016. We also set the `date` column to datetime format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsxRVz_WXgAP"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Verify the date range of the `sales` data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr2tcbBjtJLJ"
      },
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- [Use Pandas `to_datetime` function to convert dates in string format to datetime format](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZoLF6qUtJLJ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W24zZBJXgAS",
        "outputId": "89a43fa1-fadf-4e67-b947-e5819a69f43f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2016-05-22 00:00:00\n",
            "2011-01-29 00:00:00\n"
          ]
        }
      ],
      "source": [
        "# verify max and min date\n",
        "sales_data['date'] = pd.to_datetime(sales_data['date'], format='%Y-%m-%d', errors='ignore')\n",
        "print(max(sales_data['date']))\n",
        "print(min(sales_data['date']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVCHVEk2XgAU"
      },
      "source": [
        "## Processing of Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY73d0TwXgAW"
      },
      "source": [
        "There are 3 common data quality issues to check for time series data:\n",
        "1. Irregular time series\n",
        "2. Outliers\n",
        "3. Missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J65pQVGjXgAX"
      },
      "source": [
        "### Data quality issue - Irregular time series"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZABUkISXgAb"
      },
      "source": [
        "In our case, our sales data is of daily frequency. However, the data may possibly miss out some dates (see point 1 in image below) or added more rows for the same date (see point 2 in image below). Hence, we need to ensure that every date is accounted by exactly one row. \n",
        "\n",
        "For the scenario of point 2 in image below, we will also need to check with the data owner on the proper treatment, i.e., do we add up the sales of both rows to get sales for 2011-02-19 or do we just keep the row with time 00:00:00 because the data is a snapshot of sales? \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP2_pHJlXgAe"
      },
      "source": [
        "![Irregular Time Series](https://s3.ap-southeast-1.amazonaws.com/www.jiahao.io/manning/irregular_time_period.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOkqWo8NXgAh"
      },
      "source": [
        "<ins>Instruction</ins>:<br>\n",
        "- Check for duplicated dates within a store and category group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjM-hM4rtJLK"
      },
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- [Use Pandas `duplicated` method to check for duplicates](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "blVp1mJQXgAh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "010f8077-547d-4d4e-bd1d-72ad8a65cc3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# check for duplicate dates within a store and category group\n",
        "sum(sales_data[['date', 'store_id', 'cat_id']].duplicated())==0\n",
        "# how to combine the result?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XI1kbceXgAh"
      },
      "source": [
        "You should find that there is no duplicated date in a store and category group.\n",
        "\n",
        "Next let's try to see if there is missing date in between the start and end date of sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV5FvTHZXgAj"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Generate dataframe with the daily dates in between the start and end dates of sales. Then check if there is any missing date(s) in between."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NawD8dDUtJLK"
      },
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- [Use Pandas `date_range` function to generate dates in a specified time period](https://pandas.pydata.org/docs/reference/api/pandas.date_range.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN3rW5tQXgAk",
        "outputId": "d5b54698-72ba-487f-aad6-8aaf1ca576c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatetimeIndex(['2011-01-29', '2011-01-30', '2011-01-31', '2011-02-01',\n",
              "               '2011-02-02', '2011-02-03', '2011-02-04', '2011-02-05',\n",
              "               '2011-02-06', '2011-02-07',\n",
              "               ...\n",
              "               '2016-05-13', '2016-05-14', '2016-05-15', '2016-05-16',\n",
              "               '2016-05-17', '2016-05-18', '2016-05-19', '2016-05-20',\n",
              "               '2016-05-21', '2016-05-22'],\n",
              "              dtype='datetime64[ns]', length=1941, freq='D')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# generate daily dates between the start and end of sales\n",
        "# merge the dates dataframe with sales dataframe to check for missing dates\n",
        "min_date = min(sales_data['date'])\n",
        "max_date = max(sales_data['date'])\n",
        "pd.date_range(start=min_date, end=max_date)\n",
        "sales_data[['date', 'store_id', 'cat_id']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkGFqL45XgAq"
      },
      "source": [
        "Luckily for us, there is no missing date in between. If there is any, the `sales` value will be null and we can impute them later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4QX3N2nXgAq"
      },
      "source": [
        "### Data quality issue - Outliers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI1__BtdXgAr"
      },
      "source": [
        "There are various ways to check for outliers in time series. Here, we use the Interquartile Range (IQR) method mentioned in the book [Forecasting: Principles and Practice](https://otexts.com/fpp3/missing-outliers.html) by Rob J Hyndman and George Athanasopoulos. However, we modified the method a little by detrending the time series first before applying the IQR method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NROlj05RXgAu"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Code the IQR method and implement it on the sales data. Take care to check for outliers by each store and category group. The sales dataframe should have a column named `anomaly` that will indicate each outlier as True and non-outliers as False.\n",
        "- Detrend the sales by each time series (i.e., each store and category group) before applying the IQR method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXpEFl9atJLL"
      },
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- [Use Pandas `quantile` method to get the quantiles values](https://pandas.pydata.org/docs/reference/api/pandas.Series.quantile.html)\n",
        "- [Scipy `detrend` can be used to detrend the time series before applying IQR method](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.detrend.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eap5QdmBXgAv"
      },
      "outputs": [],
      "source": [
        "# outlier detection function\n",
        "def outlier_detection(df):\n",
        "    \"\"\"\n",
        "    Add column 'anomaly' to dataframe to mark outliers as True, non-outliers as False. \n",
        "    \"\"\"\n",
        "\n",
        "    return df\n",
        "\n",
        "# execute outlier detection function for each time series\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNxynZStXgAw"
      },
      "source": [
        "Let's take a look at how our outlier detection method performs.\n",
        "\n",
        "In general, the method is able to capture most of the outliers (as seen from the diagrams below) that our human judgement would also determine to be outlying points. There are some outlying points that were missed out but we shall see in a while what these points may be and we can manually mark them as outlier if necessary.\n",
        "\n",
        "We also noticed that some of the points towards the tail end of the time series for the store *CA_2* and category *FOODS* seem to be wrongly marked as outliers. We can unmark these points as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLM25GqbXgAw"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Visualize the outliers identified by overlaying them on a sales line plot. An example plot is as shown below.\n",
        "\n",
        "![Sales Line Plot with Outliers Example](https://s3.ap-southeast-1.amazonaws.com/www.jiahao.io/manning/sales_line_plot_w_outliers.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05mapYXwtJLL"
      },
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- [Use Matplotlib `plot` function to draw a line plot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gq-KS8RXgAw"
      },
      "outputs": [],
      "source": [
        "# sample one of the store category to see how our outlier detection performs\n",
        "def visualize_outliers(store, category):\n",
        "    # visualization of outliers detected\n",
        "    \n",
        "\n",
        "# sample one of the store category to see how our outlier detection performs\n",
        "visualize_outliers('TX_1', 'FOODS')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrruDzZnXgA5"
      },
      "source": [
        "We can unmark some of the points as outliers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW5jeivGXgA6"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- If any of the \"outliers\" identified are incorrect, unmark them in your `anomaly` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jH0MJe5RXgA6"
      },
      "outputs": [],
      "source": [
        "# unmark points as outliers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UROc6NU8XgBS"
      },
      "source": [
        "It turns out that when we filter and zoom into those outlying points with sales very close to 0, they seem to generally occur on 25th Dec. Seems likely due to Christmas... Turns out when you spoke to the data owner, it was revealed that the stores close on Christmas hence the close to zero sales.\n",
        "\n",
        "Let's mark these as outliers for the time being although we will need to take care to set predicted sales on Christmas to zero later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGTAxRUYXgBT"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Mark any additional points as outliers if necessary in your `anomaly` column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxNnxbfNXgBT"
      },
      "outputs": [],
      "source": [
        "# mark points as outliers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCvnu2wLXgBZ"
      },
      "source": [
        "Finally, we will need to set our outliers to null sales value. In reality, we should talk to the data owner to find out the reasons for these outliers and decide if they are legitimate data points (e.g. the Christmas points) or erroneous points.\n",
        "\n",
        "For the Christmas data points, they reflect the calendar effect and in some models, we can leave these data points as they are and add in covariates indicating these calendar effects into the model. However for us, we will be using univariate model and it would better for us to remove these calendar effects.\n",
        "\n",
        "Before we set the outliers to null sales value, we can check for other null sales values if any and perform imputation in our next data quality check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQfpdGnMXgBa"
      },
      "source": [
        "### Data quality issue - Missing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LImM5ssHXgBa"
      },
      "source": [
        "First let's check for missing sales value. Then we proceed to set outlier to null sales value as mentioned in previous data quality check. And finally perform imputation of sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NySUqAf-XgBa"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Check how many rows have missing sales value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5c5fgZmtJLM"
      },
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- [Use Pandas `isna` method to check number of missing rows](https://pandas.pydata.org/docs/reference/api/pandas.Series.isna.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRSmXPzGXgBa"
      },
      "outputs": [],
      "source": [
        "# check number of rows with missing sales\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnMG31okXgBb"
      },
      "source": [
        "Good, seems like no missing sales. Well in fact, we knew there's no missing sales when we were previewing the data. But this is just for procedure.\n",
        "\n",
        "Next we can set our outliers to null sales value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsH5-JJVXgBb"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Set outliers to null sales value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l65VabmGXgBd"
      },
      "outputs": [],
      "source": [
        "# set outlier to null sales value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq4n9AUWXgBg"
      },
      "source": [
        "We can now proceed to impute the missing sales. In our case, we have no extended missing sales period and the number of data points with missing sales is little compared to the total number of data points. Hence, we shall adopt linear interpolation method.\n",
        "\n",
        "First, we need to set the `date`, `store_id` and `cat_id` columns as index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOnH3JOKXgBg"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Impute the missing sales using linear interpolation method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqVAjkKktJLN"
      },
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- [Use Pandas `interpolate` method to impute using linear interpolation method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.interpolate.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uFodXDzXgBg"
      },
      "outputs": [],
      "source": [
        "# impute missing sales\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcMj_UojXgBg"
      },
      "source": [
        "With these data quality checks done, we have completed Milestone 1 and prepared our preprocessed data. Good job!\n",
        "\n",
        "Now let's save our preprocessed data as `sales_processed.csv` in the data folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qetS6b3XgBj"
      },
      "source": [
        "<ins>Instructions</ins>:<br>\n",
        "- Save the preprocessed data as `sales_processed.csv` in the data folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oel5feawtJLO"
      },
      "source": [
        "<ins>Hints</ins> (click when needed):<br>\n",
        "- [Use Pandas `to_csv` method to save your output as CSV file](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thfGUShkXgBj"
      },
      "outputs": [],
      "source": [
        "# write to CSV\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po2wO_CPtJLO"
      },
      "source": [
        "Do check your answers against our solution before moving on to the next Milestone."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Prepare_and_Analyze_Time_Series_Milestone_1_Workflow.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "b7bc2e76cbe5fb8595f4ba211d5aadd6fa3b8e13e3681bdb0f26937058245569"
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 64-bit ('manning': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}